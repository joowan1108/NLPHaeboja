{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfgHhPSlR7Ez1bd8ooBRMO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#FastText: 내부 단어도 고려하는 word embedding\n","\n","Word2Vec의 확장판이다. Word2Vec은 하나의 단어를 쪼갤 수 없는 단위로 생각하지만 FastText는 하나의 단어 내부를 여러 단어로 나눈다.\n","\n","== 단어 하나를 n-gram을 통해 여러 subword으로 나눈다\n","\n","n=3일 때, apple -> <ap, app, ppl, ple, le>로 나눈다. (< 와 >은 시작과 끝을 의미한다.)\n"," 이 내부 subword들을 하나의 토큰으로 인식하여 벡터로 만든다. 또, 여기서 추가로 하나를 더 벡터화하는데, 기존 단어에 <와 >을 붙인 < apple >도 토큰으로 만든다.\n","\n"," 정리해서, apple이라는 단어를 벡터화할 때는, <ap + app + ppl + ple + le>를 더한 값을 이용한다."],"metadata":{"id":"RH_0zPG0bGHr"}},{"cell_type":"markdown","source":["#Word2Vec와 비교하였을 때 얻을 수 있는 강점 1.\n","\n","Out of vocabulary에 대한 대응 성능이 좋아진다.\n","\n","ex) Word2Vec는 birthplace라는 단어가 vocab에 없을 때, 이를 그냥 OOV(out of vocabulary)로 처리한다. 하지만 FastText는 birthplace를 birth + place라는 내부 단어로 나눌 수 있으므로 birthplace라는 단어가 vocab에 없어도 이를 처리할 수 있다.\n","\n","#Word2Vec와 비교하였을 때 얻을 수 있는 강점 2.\n","\n","Word2Vec은 등장 빈도 수가 적은 단어에 대해서 weight update를 자주 할 수 없기 때문에 임베딩의 정확도가 높지 않다. 하지만 FastText는 등장 빈도 수가 적은 단어의 subword가 등장 빈도 수가 많은 단어의 subword와 곂친다면, 임베딩을 비교적 더 좋게 할 수 있다. 이런 이유 때문에, 맞춤법이 잘 지켜지지 않은 corpus에 대해서는 fasttext가 성능이 더 좋다.\n"],"metadata":{"id":"UyrY4tyJcOxK"}},{"cell_type":"markdown","source":["#Word2Vec와 FastText 비교"],"metadata":{"id":"jUW3uT6TcNng"}},{"cell_type":"code","source":["pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2bMg-G-dYG4","executionInfo":{"status":"ok","timestamp":1709723468334,"user_tz":-540,"elapsed":7300,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"894660cd-a11d-42b3-9e4d-836c69d0e18c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wwnoSuoXaK3","executionInfo":{"status":"ok","timestamp":1709723471739,"user_tz":-540,"elapsed":1002,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"f3964e8d-89f4-421b-d8c6-e85992221605"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ted_en-20160408.xml', <http.client.HTTPMessage at 0x79b603648b50>)"]},"metadata":{},"execution_count":12}],"source":["import re\n","import urllib.request\n","import zipfile\n","from lxml import etree\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","# 데이터 다운로드\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n"]},{"cell_type":"code","source":["targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n","target_text = etree.parse(targetXML)\n","\n","# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n","parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n","\n","# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n","# 해당 코드는 괄호로 구성된 내용을 제거.\n","content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n","\n","# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n","sent_text = sent_tokenize(content_text)\n","\n","# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n","normalized_text = []\n","for string in sent_text:\n","     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n","     normalized_text.append(tokens)\n","\n","# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n","result = [word_tokenize(sentence) for sentence in normalized_text]\n","\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","\n","model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n","\n"],"metadata":{"id":"Z7NH2nxtX4vG","executionInfo":{"status":"ok","timestamp":1709723585744,"user_tz":-540,"elapsed":87853,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model.wv.most_similar(\"electrofishing\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"id":"EJ75Ls96d-hh","executionInfo":{"status":"error","timestamp":1709723626488,"user_tz":-540,"elapsed":393,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"3617d9ba-548c-4a81-e7ad-e34f39b22267"},"execution_count":14,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"\"Key 'electrofishing' not present in vocabulary\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-f9544fe3d5e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Key 'electrofishing' not present in vocabulary\""]}]},{"cell_type":"code","source":["from gensim.models import FastText\n","\n","model = FastText(result, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n"],"metadata":{"id":"smi4Z7I7eFIX","executionInfo":{"status":"ok","timestamp":1709723990149,"user_tz":-540,"elapsed":220752,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model.wv.most_similar(\"electrofishing\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOKHtf_IfrK9","executionInfo":{"status":"ok","timestamp":1709724057003,"user_tz":-540,"elapsed":381,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"24d3c24a-b061-44d7-ad87-c1082f233687"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('electrolux', 0.8728017807006836),\n"," ('electrolyte', 0.8689448833465576),\n"," ('electro', 0.8544501066207886),\n"," ('electroshock', 0.851844072341919),\n"," ('electroencephalogram', 0.8349860906600952),\n"," ('electronic', 0.8321779370307922),\n"," ('electrogram', 0.8268095850944519),\n"," ('electrochemical', 0.8254574537277222),\n"," ('electron', 0.8214612007141113),\n"," ('electric', 0.8171874284744263)]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":[],"metadata":{"id":"6xNM9Nb2fwzB"},"execution_count":null,"outputs":[]}]}