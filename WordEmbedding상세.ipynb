{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1CaOgGAztzPmoPDjE9-NJCe2AdybJyOdP","authorship_tag":"ABX9TyMNg8/ewJ3QNk7iePBGHjD/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#임베딩 벡터 사용 방법 1: 임베딩 테이블 (Layer) 만들기\n","\n","nn.Embedding()을 톹해 구현"],"metadata":{"id":"lH234gp5i53G"}},{"cell_type":"markdown","source":["#임베딩 Layer = 룩업 테이블\n","\n","Word Embedding 과정\n","\n","단어 -> 고유한 정수 부여 -> 임베딩 layer와 내적 -> 임베딩 벡터\n","\n","임베딩 layer의 각 행은 vocab에 있는 단어 하나와 대응한다. 임베딩 layer의 row = vocab 길이, column = embedding vector의 차원\n","\n","\n"],"metadata":{"id":"om1b8RBVkxnQ"}},{"cell_type":"markdown","source":["#파이썬으로만 구현\n"],"metadata":{"id":"XFrHGVpolXvf"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzldQJv2eRML","executionInfo":{"status":"ok","timestamp":1709799615183,"user_tz":-540,"elapsed":6,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"743e2270-b332-4a4c-d450-1b51f245547e"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'you': 2, 'how': 3, 'to': 4, 'know': 5, 'need': 6, 'code': 7, '<unk>': 0, '<pad>': 1}\n"]}],"source":["train_data = \"you need to know how to code\"\n","\n","#vocab 만들기\n","word_set = set(train_data.split())\n","\n","#정수 mapping\n","vocab = {word: i+2 for i, word in enumerate(word_set)}\n","vocab['<unk>'] = 0\n","vocab['<pad>'] = 1\n","print(vocab)"]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"PhlfN0J0mAWm","executionInfo":{"status":"ok","timestamp":1709799616983,"user_tz":-540,"elapsed":3,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#임베딩 벡터의 차원을 3, vocab 길이만큼 행 설정\n","embedding_table = torch.FloatTensor([\n","                               [ 0.0,  0.0,  0.0],\n","                               [ 0.0,  0.0,  0.0],\n","                               [ 0.2,  0.9,  0.3],\n","                               [ 0.1,  0.5,  0.7],\n","                               [ 0.2,  0.1,  0.8],\n","                               [ 0.4,  0.1,  0.1],\n","                               [ 0.1,  0.8,  0.9],\n","                               [ 0.6,  0.1,  0.1]])"],"metadata":{"id":"RJZ5loj_l5Jb","executionInfo":{"status":"ok","timestamp":1709799618857,"user_tz":-540,"elapsed":5,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["sample = 'you need to run'.split()\n","indexes = []\n","\n","for word in sample:\n","  try:\n","    indexes.append(vocab[word])\n","  except KeyError:\n","    indexes.append(vocab['<unk>'])\n","indexes = torch.LongTensor(indexes)\n","\n","lookup_table = embedding_table[indexes, :]\n","print(lookup_table)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1j9kpJOSmc-I","executionInfo":{"status":"ok","timestamp":1709799621028,"user_tz":-540,"elapsed":7,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"2eef0c09-15cc-466e-d3be-1039c50ed01c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2000, 0.9000, 0.3000],\n","        [0.1000, 0.8000, 0.9000],\n","        [0.2000, 0.1000, 0.8000],\n","        [0.0000, 0.0000, 0.0000]])\n"]}]},{"cell_type":"markdown","source":["#nn.Embedding 사용하기"],"metadata":{"id":"4cPARM7AnRiT"}},{"cell_type":"code","source":["train_data = 'you need to know how to code'\n","\n","word_set = set(train_data.split())\n","\n","vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}\n","vocab['<unk>'] = 0\n","vocab['<pad>'] = 1\n","\n","import torch.nn as nn\n","embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=3, padding_idx=1)\n"],"metadata":{"id":"30GccMtfnJnt","executionInfo":{"status":"ok","timestamp":1709799623414,"user_tz":-540,"elapsed":4,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(embedding_layer.weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iw-TQRRYobNi","executionInfo":{"status":"ok","timestamp":1709799627092,"user_tz":-540,"elapsed":21,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"a220ab81-2cea-4836-9680-0436b513d3e3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[-1.1858, -0.3080, -0.3831],\n","        [ 0.0000,  0.0000,  0.0000],\n","        [-1.2776,  0.3546, -0.1265],\n","        [ 0.1807, -0.4749,  0.9652],\n","        [-0.9273, -0.3804,  1.8888],\n","        [-0.0056, -0.5643, -0.2401],\n","        [ 1.6561, -1.2417, -0.5832],\n","        [ 0.6983,  0.5729,  0.3822]], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["#임베딩 벡터 사용 방법 2: Pre Trained Word Embedding\n","\n","학습 데이터가 많지 않다면, 직접 layer를 구하는 것보다 이미 학습된 임베딩 벡터들을 사용하는 것이 낫다.\n","\n","직접 모델을 구현하고 사전 훈련된 워드 임베딩과 구현해보자"],"metadata":{"id":"Q5j8BQIboja0"}},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"xV5XeBzBoi9q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709799634731,"user_tz":-540,"elapsed":6346,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"4376e12f-0a9c-4190-b3fa-bbd19fa594ac"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","import gensim"],"metadata":{"id":"xqSluc1k5qR4","executionInfo":{"status":"ok","timestamp":1709799638295,"user_tz":-540,"elapsed":1507,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["문장의 긍부적을 판단하는 감성 분류 모델: Positive: 1 Negative: 0\n"],"metadata":{"id":"cWHpdcxJ52zd"}},{"cell_type":"code","source":["sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n","y_train = [1, 0, 0, 1, 1, 0, 1]\n"],"metadata":{"id":"E7H2gFX25-f5","executionInfo":{"status":"ok","timestamp":1709799640269,"user_tz":-540,"elapsed":3,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["tokenized_sentences = [line.split() for line in sentences]\n","print(\"단어 토큰화 결과: \", tokenized_sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8ypRDn_6DoD","executionInfo":{"status":"ok","timestamp":1709799641702,"user_tz":-540,"elapsed":4,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"543e1ffb-9cbf-4064-831f-9824fcac8d03"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 토큰화 결과:  [['nice', 'great', 'best', 'amazing'], ['stop', 'lies'], ['pitiful', 'nerd'], ['excellent', 'work'], ['supreme', 'quality'], ['bad'], ['highly', 'respectable']]\n"]}]},{"cell_type":"markdown","source":["단어 집합 만들어보자"],"metadata":{"id":"JpDwXola6POL"}},{"cell_type":"code","source":["vocab = []\n","for line in tokenized_sentences:\n","  for word in line:\n","    vocab.append(word)\n","\n","word_counts = Counter(vocab)\n","print(\"총 단어 수: \",len(word_counts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwltHhPk6ONA","executionInfo":{"status":"ok","timestamp":1709799643549,"user_tz":-540,"elapsed":5,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"2294019d-c9a9-4997-ae2b-404b62ba5674"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["총 단어 수:  15\n"]}]},{"cell_type":"markdown","source":["단어들을 등장 빈도가 높은 순서부터 정렬"],"metadata":{"id":"D3m9F7ZD6mrV"}},{"cell_type":"code","source":["vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tpsv34Wk6fBC","executionInfo":{"status":"ok","timestamp":1709799644987,"user_tz":-540,"elapsed":4,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"0f650a37-b05b-4306-c3fc-023b60a676c6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["['nice', 'great', 'best', 'amazing', 'stop', 'lies', 'pitiful', 'nerd', 'excellent', 'work', 'supreme', 'quality', 'bad', 'highly', 'respectable']\n"]}]},{"cell_type":"code","source":["word_to_index = {}\n","word_to_index['<PAD>'] = 0\n","word_to_index['<UNK>'] = 1\n","\n","for index, word in enumerate(vocab):\n","  word_to_index[word] = index+2\n","\n","vocab_size = len(word_to_index)\n","print(\"패딩 토큰, UNK 토큰을 고려한 단어 집합의 크기: \", vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzTOhY0v6kc8","executionInfo":{"status":"ok","timestamp":1709799647028,"user_tz":-540,"elapsed":6,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"b9ae5fec-ee38-4f41-d8bd-0fcf5e4b4bb2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["패딩 토큰, UNK 토큰을 고려한 단어 집합의 크기:  17\n"]}]},{"cell_type":"code","source":["print(word_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmRRE3rb7CYD","executionInfo":{"status":"ok","timestamp":1709799648816,"user_tz":-540,"elapsed":7,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"2e703b09-0c92-413b-cbc6-89650d10de35"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"]}]},{"cell_type":"code","source":["def text_to_sequences(train_data, word_to_index):\n","  encoded_train_data = []\n","  for line in train_data:\n","    index_sequences = []\n","    for word in line:\n","      try:\n","        index_sequences.append(word_to_index[word])\n","      except KeyError:\n","        index_sequences.append(word_to_index['<UNK>'])\n","    encoded_train_data.append(index_sequences)\n","  return encoded_train_data\n","\n","train_data_encoded = text_to_sequences(tokenized_sentences, word_to_index)\n","print(train_data_encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yy2Gxnl47I52","executionInfo":{"status":"ok","timestamp":1709799650614,"user_tz":-540,"elapsed":6,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"bd823378-b4f0-40e1-d517-fac9cf6e1676"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]\n"]}]},{"cell_type":"markdown","source":["최대 길이를 base로 padding하기"],"metadata":{"id":"KHW8m2j284pC"}},{"cell_type":"code","source":["max_len = max(len(l) for l in train_data_encoded)\n","print(max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"db95jEc48Wu6","executionInfo":{"status":"ok","timestamp":1709799651947,"user_tz":-540,"elapsed":5,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"58148f53-2f62-4aac-c80d-43b0c887e6a2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}]},{"cell_type":"markdown","source":["임베딩 벡터 패딩하여 임베딩 벡터 최종 결과 구하자."],"metadata":{"id":"d42VsSYBCZil"}},{"cell_type":"code","source":["def pad_sequences(sentences, max_len):\n","  #각 문장에 대해 max_len만큼의 0으로 이루어진 numpy array 형성\n","  features = np.zeros((len(sentences), max_len), dtype=int)\n","  for index, sentence in enumerate(sentences):\n","    #sentence 존재한다면,\n","    if len(sentence) != 0:\n","      #그 sentence에 대응하는 numpy line에 sentence값을 넣는다\n","      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n","  return features\n","\n","X_train = pad_sequences(train_data_encoded, max_len=max_len)\n","print(\"패딩 결과: \")\n","print(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5w0vmGo9I_X","executionInfo":{"status":"ok","timestamp":1709799653837,"user_tz":-540,"elapsed":5,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"1889a260-675e-40ad-9b97-efb18f78b0ba"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["패딩 결과: \n","[[ 2  3  4  5]\n"," [ 6  7  0  0]\n"," [ 8  9  0  0]\n"," [10 11  0  0]\n"," [12 13  0  0]\n"," [14  0  0  0]\n"," [15 16  0  0]]\n"]}]},{"cell_type":"markdown","source":["임베딩 벡터를 이용하여 감정 분류하는 Binary Classifier simple model 만들기\n"],"metadata":{"id":"5R-BmQIM-Ebl"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","class SimpleModel(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super(SimpleModel, self).__init__()\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.flatten = nn.Flatten()\n","    self.fc = nn.Linear(embedding_dim * max_len, 1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self,x):\n","    embedded = self.embedding(x)\n","\n","    flattened = self.flatten(embedded)\n","\n","    output = self.fc(flattened)\n","    return self.sigmoid(output)\n"],"metadata":{"id":"Sh0sX-ps-I92","executionInfo":{"status":"ok","timestamp":1709799656063,"user_tz":-540,"elapsed":4,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","embedding_dim = 100\n","simple_model = SimpleModel(vocab_size, embedding_dim).to(device)\n"],"metadata":{"id":"6pWpt_3o_jNU","executionInfo":{"status":"ok","timestamp":1709799658974,"user_tz":-540,"elapsed":4,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["criterion = nn.BCELoss()\n","optimizer = Adam(simple_model.parameters())\n"],"metadata":{"id":"x4uSHD-WA3vD","executionInfo":{"status":"ok","timestamp":1709799894648,"user_tz":-540,"elapsed":2050,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\n","train_dataloader = DataLoader(train_dataset, batch_size=2)"],"metadata":{"id":"MNRy6yfMA_Bv","executionInfo":{"status":"ok","timestamp":1709799989992,"user_tz":-540,"elapsed":5,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["print(len(train_dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeK0DFvnBXEk","executionInfo":{"status":"ok","timestamp":1709800002380,"user_tz":-540,"elapsed":7,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"3bea5cd5-fba7-4d8b-95cd-ea9299758df0"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}]},{"cell_type":"code","source":["for epoch in range(10):\n","  for inputs, targets in train_dataloader:\n","    inputs, targets = inputs.to(device), targets.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    outputs = simple_model(inputs).view(-1)\n","\n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","  print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNYhHVOMBZ4N","executionInfo":{"status":"ok","timestamp":1709800095051,"user_tz":-540,"elapsed":1176,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"69662fe3-3fcd-47f5-c4b8-9e7f5c991061"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.4546612501144409\n","Epoch 2, Loss: 0.4116010069847107\n","Epoch 3, Loss: 0.3368053734302521\n","Epoch 4, Loss: 0.26792916655540466\n","Epoch 5, Loss: 0.21520820260047913\n","Epoch 6, Loss: 0.17791545391082764\n","Epoch 7, Loss: 0.15205876529216766\n","Epoch 8, Loss: 0.1336413472890854\n","Epoch 9, Loss: 0.11963801831007004\n","Epoch 10, Loss: 0.10808978229761124\n"]}]},{"cell_type":"markdown","source":["사전 훈련된 임베딩을 사용하자\n","\n","밑에는 구글에서 사전 학습시킨 Word2Vec 모델이다"],"metadata":{"id":"aru5QlKRC_iv"}},{"cell_type":"code","source":["!pip install gdown\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sy3HOPZOCP_7","executionInfo":{"status":"ok","timestamp":1709801074795,"user_tz":-540,"elapsed":6379,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"f8f524c8-33ad-493e-8a99-aae9d991632a"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}]},{"cell_type":"code","source":["# 구글의 사전 훈련된 Word2vec 모델을 로드합니다.\n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Pytorch_Study/자연어처리pytorch/GoogleNews-vectors-negative300.bin.gz', binary=True)\n","#구글의 Word2Vec은 embedding vector를 300차원으로 구성하였다."],"metadata":{"id":"TO-HeCwcDKtQ","executionInfo":{"status":"ok","timestamp":1709801168490,"user_tz":-540,"elapsed":58301,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["비어있는 행렬을 만들자. 이걸로 google word2vec의 embedding vector를 알아내보자. vocab에 존재하는 각 단어의 embedding vector를 저장할 것이다."],"metadata":{"id":"TVTg5BhkHQ99"}},{"cell_type":"code","source":["embedding_matrix = np.zeros((vocab_size, 300))\n","print(\"임베딩 행렬의 크기: \", embedding_matrix.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceHT2UyWFY3n","executionInfo":{"status":"ok","timestamp":1709801253776,"user_tz":-540,"elapsed":6,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"1f2f41fe-b52e-48a4-ad55-c43e0c6b6a26"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["임베딩 행렬의 크기:  (17, 300)\n"]}]},{"cell_type":"markdown","source":["word2vec_model은 특정 단어를 입력하면 해당 단어의 임베딩 벡터를 리턴받을텐데, 이 모델에 특정 단어가 존재하지 않는다면, none을 리턴하도록 한다."],"metadata":{"id":"NasKwZpYGL6L"}},{"cell_type":"code","source":["def get_vector(word):\n","  if word in word2vec_model:\n","    return word2vec_model[word]\n","  else:\n","    return None"],"metadata":{"id":"6bGcDfKwGKoU","executionInfo":{"status":"ok","timestamp":1709801394371,"user_tz":-540,"elapsed":702,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["for word, i in word_to_index.items():\n","  if i>2:\n","    temp = get_vector(word)\n","    if temp is not None:\n","      embedding_matrix[i] = temp"],"metadata":{"id":"pLX_0QA_GsVn","executionInfo":{"status":"ok","timestamp":1709801442914,"user_tz":-540,"elapsed":576,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["embedding_matrix"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6j5Wnf6G5WC","executionInfo":{"status":"ok","timestamp":1709801451578,"user_tz":-540,"elapsed":5,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"d9c2fe60-fd60-4aa8-ef2b-f4f72616810a"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n","         0.        ,  0.        ],\n","       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n","         0.        ,  0.        ],\n","       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n","         0.        ,  0.        ],\n","       ...,\n","       [ 0.06298828,  0.12451172,  0.11328125, ..., -0.06347656,\n","         0.11474609,  0.03100586],\n","       [ 0.05078125, -0.22753906, -0.13085938, ..., -0.05664062,\n","         0.10253906,  0.07470703],\n","       [ 0.15039062,  0.28515625, -0.02319336, ..., -0.20019531,\n","        -0.11230469,  0.10302734]])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["word_to_index['great']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"deD8w8Y8HhTX","executionInfo":{"status":"ok","timestamp":1709801637569,"user_tz":-540,"elapsed":534,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"e0ed0131-4c9f-44b1-f9f3-76843e43d135"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["np.all(word2vec_model['great'] == embedding_matrix[3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdRNZwqlHoxk","executionInfo":{"status":"ok","timestamp":1709801667566,"user_tz":-540,"elapsed":6,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"a69c6321-6fc3-43ec-efc3-fa9126c83b31"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["이 사전 훈련 모델을 이용한 Binary Classifier를 구성해보자"],"metadata":{"id":"jZSmuWOaHyKm"}},{"cell_type":"code","source":["class PretrainedEmbeddingModel(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super(PretrainedEmbeddingModel, self).__init__()\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n","    self.embedding.weight.requires_grad = True\n","    self.flatten = nn.Flatten()\n","    self.fc = nn.Linear(embedding_dim*max_len, 1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    embedded = self.embedding(x)\n","    flattened = self.flatten(embedded)\n","    output = self.fc(flattened)\n","    return self.sigmoid(output)\n","\n"],"metadata":{"id":"isVNOJ7sHxfX","executionInfo":{"status":"ok","timestamp":1709801925869,"user_tz":-540,"elapsed":490,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["pretrained_embedding_model = PretrainedEmbeddingModel(vocab_size, 300).to(device)"],"metadata":{"id":"d829PDkLIj3R","executionInfo":{"status":"ok","timestamp":1709801927835,"user_tz":-540,"elapsed":4,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["criterion = nn.BCELoss()\n","optimizer = Adam(pretrained_embedding_model.parameters())\n"],"metadata":{"id":"k3OkKtd5Iv4S","executionInfo":{"status":"ok","timestamp":1709801964205,"user_tz":-540,"elapsed":5,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\n","train_dataloader = DataLoader(train_dataset, batch_size=2)"],"metadata":{"id":"cMNIDd9HI1hA","executionInfo":{"status":"ok","timestamp":1709802014486,"user_tz":-540,"elapsed":6,"user":{"displayName":"주완채","userId":"14152143105854180609"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["for epoch in range(10):\n","  for inputs, targets in train_dataloader:\n","    inputs, targets = inputs.to(device), targets.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    outputs = pretrained_embedding_model(inputs).view(-1)\n","\n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","  print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwU8gH4PJFw6","executionInfo":{"status":"ok","timestamp":1709802090916,"user_tz":-540,"elapsed":8,"user":{"displayName":"주완채","userId":"14152143105854180609"}},"outputId":"0d689523-cdfa-4887-fd9b-d90e6a177e2a"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.7178673148155212\n","Epoch 2, Loss: 0.6515899896621704\n","Epoch 3, Loss: 0.5859054923057556\n","Epoch 4, Loss: 0.5242006778717041\n","Epoch 5, Loss: 0.46734774112701416\n","Epoch 6, Loss: 0.41553616523742676\n","Epoch 7, Loss: 0.3686983585357666\n","Epoch 8, Loss: 0.3266470730304718\n","Epoch 9, Loss: 0.28912681341171265\n","Epoch 10, Loss: 0.25583571195602417\n"]}]}]}